{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMapData Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author : Chaitanya Madala**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date : May 15, 2016**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Area\n",
    "\n",
    "[Ahmedabad, Gujarat, India](https://en.wikipedia.org/wiki/Ahmedabad)\n",
    "\n",
    "[DataSet](https://mapzen.com/data/metro-extracts/metro/ahmedabad_india/) : This Dataset which is extracted from website openstreetmap contains information about the city Ahmedabad, India"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "\n",
    "1. [DataAuditing](#DataAuditing)\n",
    "2. [Problems Encountered In Postal Codes](#postalCodes)\n",
    "2. [Problems Encountered in City Names](#cityNames)\n",
    "3. [Problems Encountered In Phone Numbers](#phoneNumbers)\n",
    "4. [Problems Encountered In Street Names](#streetNames)\n",
    "4. [Problems Encountered In Cuisine Data](#cuisineData)\n",
    "5. [Data Wrangling With DB and File Sizes](#dbFileSizes)\n",
    "6. [Further Data Exploration With MongoDB](#furtherDbMongo)\n",
    "7. [The Conclusion](#conclusion)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Data Auditing <a name=\"DataAuditing\"><a/>\n",
    "- As part of data auditing plan lets find out what are the different types of tags present in our data set, but also how many, to get the feeling on how much of which data we can expect to have in the map.\n",
    "\n",
    "- Below are required imports and constants which will be used throught the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import pymongo\n",
    "import subprocess\n",
    "import pprint\n",
    "import codecs\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "INPUT_FILENAME = 'ahmedabad_india1.osm'\n",
    "OUTPUT_DIR = 'Output_Data'\n",
    "CURR_DIR = os.getcwd()+'\\\\'\n",
    "\n",
    "if OUTPUT_DIR not in os.listdir(CURR_DIR):\n",
    "    os.makedirs(CURR_DIR + OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1, 'tag': 98131, 'node': 546085, 'nd': 634041, 'way': 81271, 'member': 2291, 'relation': 511, 'osm': 1}\n"
     ]
    }
   ],
   "source": [
    "def count_tags(filename): \n",
    "   \n",
    "    '''This function is written to count no of \n",
    "       different tags present in the given dataset'''\n",
    "    \n",
    "    dict_tags = {}\n",
    "    for event,element in ET.iterparse(filename):\n",
    "        tag = element.tag\n",
    "        if tag in dict_tags:\n",
    "            dict_tags[tag] += 1\n",
    "        else:\n",
    "            dict_tags[tag] = 1\n",
    "            \n",
    "    return dict_tags\n",
    "\n",
    "tags = count_tags(INPUT_FILENAME)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now lets find out how many different users contributed to this Ahemdabad openstreetmap dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users contributed:  354\n"
     ]
    }
   ],
   "source": [
    "def count_users(filename):\n",
    "    \n",
    "    '''This function is written to count the number of distinct \n",
    "    users who contributed to the Ahemdabad Openstreetmap data'''\n",
    "    \n",
    "    users_set = set()\n",
    "    for event,element in ET.iterparse(filename):\n",
    "        tag = element.tag\n",
    "        if tag == 'node' or tag == 'relation' or tag == 'way':\n",
    "             users_set.add(element.attrib['user'])\n",
    "        element.clear()        \n",
    "    return users_set\n",
    "\n",
    "users = count_users(INPUT_FILENAME)\n",
    "print('Number of users contributed: ',len(users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before we procees the data and add it into our database, we should check \"k\" value for each tag and see if there are any potential problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lower': 96127, 'lower_colon': 1962, 'other': 35, 'problemchars': 7}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>();\\'\"?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "problem_chars_set = set()\n",
    "others_set = set()\n",
    "\n",
    "def key_type(element, keys):\n",
    "    '''This function is defined to categorize different \"k\" values'''\n",
    "    if element.tag == \"tag\":\n",
    "        tag_k_value = element.attrib['k']\n",
    "        match_lower = re.search(lower,tag_k_value)\n",
    "        match_lower_colon = re.search(lower_colon,tag_k_value)\n",
    "        match_problemchars  = re.search(problemchars,tag_k_value)\n",
    "        \n",
    "        if match_lower :\n",
    "            keys['lower'] += 1     \n",
    "        elif match_lower_colon :\n",
    "            keys['lower_colon'] += 1            \n",
    "        elif match_problemchars:\n",
    "            keys['problemchars'] += 1\n",
    "            problem_chars_set.add(tag_k_value)\n",
    "        else :\n",
    "            keys['other'] += 1\n",
    "            others_set.add(tag_k_value)\n",
    "            \n",
    "    return keys\n",
    "\n",
    "def process_tags(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for event,element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "process_tags(INPUT_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- The above data shows that there are 35 other category tags and 7 problem char tags. Now lets take a look at these problemchars tags and other category tags to identify those tags, which might be useful for database insertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average rate/kg', 'famous for'}\n"
     ]
    }
   ],
   "source": [
    "print(problem_chars_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AND_a_c', 'fuel:octane_92', 'Business', 'FIXME', 'FID_1', 'mtb:scale:imba', 'currency:INR', 'source_1', 'IR:zone', 'fuel:octane_80', 'source_2', 'is_in:iso_3166_2', 'plant:output:electricity', 'naptan:CommonName', 'fuel:octane_91', 'AND_a_nosr_p', 'name_2', 'name_1', 'mtb:scale:uphill'}\n"
     ]
    }
   ],
   "source": [
    "print(others_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the above listed tags, we can discard all of them except for \"famous for\" tag, as it has some meaningfull data associated with it i.e, it has the value of famous dish of that particular place or resturant. \n",
    "\n",
    "- Now lets find out what all different \"k\" values are present in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of k values dictionary:  203\n"
     ]
    }
   ],
   "source": [
    "def process_tags_k_val(filename):\n",
    "    '''This function is written to find out \n",
    "    different k values present in dataset'''\n",
    "    tags_k_values_dict = {}\n",
    "    final_list = list(others_set) + list(problem_chars_set)\n",
    "    for event,element in ET.iterparse(filename):\n",
    "        if element.tag == 'tag' :\n",
    "            tag_k = element.attrib['k']\n",
    "            if tag_k not in final_list:\n",
    "                if tag_k not in tags_k_values_dict:\n",
    "                    tags_k_values_dict[tag_k] = 1\n",
    "                else :\n",
    "                    tags_k_values_dict[tag_k] += 1\n",
    "                \n",
    "    return tags_k_values_dict\n",
    "\n",
    "tags = process_tags_k_val(INPUT_FILENAME)\n",
    "print(\"Length of k values dictionary: \",len(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As the length of dictionary is 203, the output will be huge, So I am writing it to a external file called **\"tags.txt\" **."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def output_data(lst,func=None,filename=None):\n",
    "    '''This function is written to write output \n",
    "    data to a file or show it on console'''\n",
    "    if filename != None:\n",
    "        filename = os.path.join(CURR_DIR+OUTPUT_DIR,filename)\n",
    "        \n",
    "        with open(filename,'w',encoding=\"utf-8\") as f:\n",
    "            if func != None:\n",
    "                for val in lst:\n",
    "                    f.write(\"{0} ----> {1}\\n\".format(val,func(val)))\n",
    "            else:\n",
    "                if type(lst) == type({}):\n",
    "                    for val in sorted(lst.keys()):\n",
    "                        f.write(\"{0} ----> {1}\\n\".format(val,lst[val]))\n",
    "                else:   \n",
    "                    for val in lst:\n",
    "                        f.write(\"{0}\\n\".format(val))\n",
    "    else : \n",
    "        if func != None:\n",
    "            for val in lst:\n",
    "                print(\"{0} ----> {1}\".format(val,func(val)))\n",
    "        else :\n",
    "            for val in lst:\n",
    "                print(\"{0}\".format(val))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_data(tags,filename=\"tags.txt\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems Encountered In Postal Codes <a name=\"postalCodes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now lets take a look at different postal codes present in the dataset to validate them against correct format of Ahemdabad postal codes.\n",
    "- This [[website]](http://www.mapsofindia.com/pincode/india/gujarat/ahmedabad/) lists out all the available postal codes of Ahemdabad, whcih are of the format **(38\\*\\*\\*\\*)** and are 6 digits in length.\n",
    "- When we take a look at different \"k\" value tags present in \"tags.txt\", we find that postal codes are defined under **\"addr:postcode\",\"postal_code\"**.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_postal_code_set = set()\n",
    "incorrect_postal_code_set = set()\n",
    "\n",
    "def validate_postal_code(code):\n",
    "    '''This function is written to validate \n",
    "    postal code aganist regular expression'''\n",
    "    validate_postal_code = re.compile(r'^38(\\d{4})$') #regular expression to validate postal codes.\n",
    "    match = re.search(validate_postal_code,code)\n",
    "    return match\n",
    "    \n",
    "def process_postal_codes(filename):\n",
    "    for event,element in ET.iterparse(filename):\n",
    "        if element.tag == 'tag':\n",
    "            tag_k = element.attrib['k']\n",
    "            if tag_k in ['addr:postcode','postal_code']:\n",
    "                tag_v = element.attrib['v'].replace(' ','')\n",
    "                \n",
    "                match = validate_postal_code(tag_v)\n",
    "                if match :\n",
    "                    correct_postal_code_set.add(tag_v)\n",
    "                else:\n",
    "                    incorrect_postal_code_set.add(tag_v)\n",
    "                                        \n",
    "process_postal_codes(INPUT_FILENAME)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['380001', '380003', '380004', '380005', '380006', '380007', '380008', '380009', '380013', '380014', '380015', '380021', '380023', '380024', '380026', '380027', '380028', '380043', '380051', '380052', '380054', '380055', '380058', '380059', '380061', '380063', '382006', '382007', '382009', '382110', '382210', '382325', '382345', '382350', '382405', '382418', '382421', '382424', '382440', '382445', '382475', '382480', '382481']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(correct_postal_code_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3', '33026', '3800013'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_postal_code_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Almost all the postal codes satisfy the regular expression, we assumed, except the above listed 3 postal codes. We might need to exempt them from database insertion as they are incorrect and doesn't have the correct format.\n",
    "\n",
    "##  Problems Encountered in City Names <a name=\"cityNames\"></a>\n",
    "\n",
    "- Now lets take a look at values present in **\"addr:city\"** tag, to check whether city name has been mentioned correctly in every city tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AHEMEDABAD', 'AHMEDABAD', 'Adalaj', 'Adalaj, Gandhinagar', 'Ahemdabad', 'Ahemedabad', 'Ahmadabad', 'Ahmedabad', 'Ahmedabad, Gujarat. India', 'Ahmedabad, Prahladnagar', 'Gandhinagar', 'Khodiyar', 'Koteshwar ,Ahmedabad', 'Maninagar', 'Naroda', 'Naroda road', 'Nava naroda', 'Nr.Vatva GIDC', 'Pembroke Pines', 'Ranip', 'Thaltej', 'ahmedabad', 'kOTARPUR ,Ahemedabad', 'medabad', 'ramdevnagar', 'ranip', 'sanand', 'अहमदाबाद, गुजरात']\n"
     ]
    }
   ],
   "source": [
    "def process_tags(filename,par_tag):\n",
    "    '''This function is written to process tags with specific \"k\" value.'''\n",
    "    tag_data_set = set()\n",
    "    for event,element in ET.iterparse(filename):\n",
    "        if element.tag == \"tag\":\n",
    "            tag_k = element.attrib['k']\n",
    "            if tag_k == par_tag:\n",
    "                tag_data_set.add(element.attrib['v'])\n",
    "    return tag_data_set\n",
    "\n",
    "def process_tags_dict(filename,par_tag):\n",
    "    '''This function is written to process tags with specific \"k\" value.'''\n",
    "    tag_data_set = {}\n",
    "    for event,element in ET.iterparse(filename):\n",
    "        if element.tag == \"tag\":\n",
    "            tag_k = element.attrib['k']\n",
    "            tag_v = element.attrib['v']\n",
    "            if tag_k == par_tag:\n",
    "                if tag_v not in tag_data_set:\n",
    "                    tag_data_set[tag_v] = 1\n",
    "                else :\n",
    "                    tag_data_set[tag_v] += 1\n",
    "    return tag_data_set\n",
    "                    \n",
    "city_names = sorted(process_tags(INPUT_FILENAME,\"addr:city\"))\n",
    "print(city_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The observations that can be drawn from the above listed city name are:\n",
    "  1. **\"Ahmedabad\"** is mispelled in various forms like **[\"AHEMEDABAD\", \"Ahemedabad\", \"Ahemdabad\", \"Ahmadabad\"].**\n",
    "  2. Instead of directly mentioning the city name, it is mentioned with either some local area or with state name like **[\"Ahmedabad,Gujarat. India\", \"Ahmedabad,Prahladnagar\", \"Koteshwar,Ahmedabad\", \"kOTARPUR,Ahemedabad\"].**\n",
    "  3. Instead of mentioning the city name, some local area is mention in city tag like **[\"Khodiyar\",\"Maninagar\",\"Naroda\",\"Naroda road\",\"Nava naroda\",\"Nr.Vatva GIDC\",\"Pembroke Pines\",\"Ranip\",\"medabad\",\"ramdevnagar\"].**\n",
    "  4. City name is mentioned in local language **Hindi** like **[\"अहमदाबाद, गुजरात\"].**\n",
    "  5. As Ahmedabad is situated near by Gandhinagar, some of the city tags has city value as **[\"Gandhinagar\", \"Adalaj, Gandhinagar\", \"Adalaj\"].** \n",
    "  \n",
    "- Now lets write a function that would written correct value of city which would be useful for processing city names at time of database insertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rectify_city_name(city_name):\n",
    "    '''This function is written to rectify a given city name'''\n",
    "    validate_city_ahmedabad = re.compile(r'Ah(.)*daba(d|d\\,)',re.IGNORECASE)\n",
    "    validate_city_gandhinagar = re.compile(r'(gandhinaga(r|r\\,))|(Adalaj)',re.IGNORECASE)\n",
    "    result = None\n",
    "    if re.search(validate_city_ahmedabad,city_name):\n",
    "        result = \"Ahmedabad\"\n",
    "    elif re.search(validate_city_gandhinagar,city_name):\n",
    "        result = \"Gandhinagar\"\n",
    "    else :\n",
    "        result = \"Ahmedabad\"\n",
    "     \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AHEMEDABAD ----> Ahmedabad\n",
      "AHMEDABAD ----> Ahmedabad\n",
      "Adalaj ----> Gandhinagar\n",
      "Adalaj, Gandhinagar ----> Gandhinagar\n",
      "Ahemdabad ----> Ahmedabad\n",
      "Ahemedabad ----> Ahmedabad\n",
      "Ahmadabad ----> Ahmedabad\n",
      "Ahmedabad ----> Ahmedabad\n",
      "Ahmedabad, Gujarat. India ----> Ahmedabad\n",
      "Ahmedabad, Prahladnagar ----> Ahmedabad\n",
      "Gandhinagar ----> Gandhinagar\n",
      "Khodiyar ----> Ahmedabad\n",
      "Koteshwar ,Ahmedabad ----> Ahmedabad\n",
      "Maninagar ----> Ahmedabad\n",
      "Naroda ----> Ahmedabad\n",
      "Naroda road ----> Ahmedabad\n",
      "Nava naroda ----> Ahmedabad\n",
      "Nr.Vatva GIDC ----> Ahmedabad\n",
      "Pembroke Pines ----> Ahmedabad\n",
      "Ranip ----> Ahmedabad\n",
      "Thaltej ----> Ahmedabad\n",
      "ahmedabad ----> Ahmedabad\n",
      "kOTARPUR ,Ahemedabad ----> Ahmedabad\n",
      "medabad ----> Ahmedabad\n",
      "ramdevnagar ----> Ahmedabad\n",
      "ranip ----> Ahmedabad\n",
      "sanand ----> Ahmedabad\n",
      "अहमदाबाद, गुजरात ----> Ahmedabad\n"
     ]
    }
   ],
   "source": [
    "output_data(city_names,func=rectify_city_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems Encountered In Phone Numbers <a name=\"phoneNumbers\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now lets take look at values present in **\"phone\"** tag to check whether they are correct format or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'079 26920057', '+91 79 2589 4542 / +91 9429207992', '+917927472043', '+91 79 3983 0100 ', '855-553-4767', '+917922864345', '+917922700585', '+91-9978113275 ; +91-8390740897', '07927641100', '9375776800', '+917965469992', '079 6619 0201', '+91 98981 37147', '(+91-79) 4032-7226', '07925500007', '+91793013 0200', '07926304000', '+917927550875', '07922912990', '+9179 2657 8369', '+91 79 6190 0500/05/06/07/08/09', '+91 98250 41132', '079 4050 5050', '+91 8758637922', '079 2687 2386', '0792740 0228', '+917927506819', '+919375565533', '915752790', '+91 79 2657 7621', '+91 79 2550 7181', '+919099958936', '+91 94262 84715', '+91 79 2656 5222', '07926306752', '7096805450', '+91 79 2646 6464', '+91 99-98-264810', '093270 38242', '+91 79 29705588', '+917923224006', '+91 93776 19151', '07926582130', '07922720605', '+91 9054876866', '099099 22239', '+91 79 30912345', '+91 79 2657 5741', '09016861000', '7926620059', '+919879566257', '07965422223', '+91 79 6651 5151', '+917801949128', '+91 79 25556767', '9909005694', '91-79-26401554', '+917922167530', '917926314000', '(079)39830036/37'}\n"
     ]
    }
   ],
   "source": [
    "phone_numbers = process_tags(INPUT_FILENAME,\"phone\")\n",
    "print(phone_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The observations that can be drawn from the above list phone numbers are:\n",
    "  1. Some of the phone numbers are starting with coutry code **\"+91\" or 91** like **+91 79 2550 7181, \"917926314000\" .**\n",
    "  2. some of the phone numbers are starting with **Zero** like **\"079 6619 0201\".**\n",
    "  3. Some numbers are having Parentheses in them like **\"(+91-79) 4032-7226\".** \n",
    "  4. Some numbers are standered **ten digit** phone numbers like **\"7926620059\".**\n",
    "  5. Some places have multiple phone numbers like **\"+91 79 6190 0500/05/06/07/08/09\".**\n",
    "  6. Some numbers are having incorrect format like **\"915752790\".**\n",
    "  \n",
    "- Now lets write a function to convert all the phone numbers to **standard format** like **\"+91 88 7777 6666\"**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rectify_phone_number(phone_number):\n",
    "    '''This function is written to rectify a given phone number'''\n",
    "    detect_multiple = re.compile(r'[/;]')\n",
    "    match = re.search(detect_multiple,phone_number)\n",
    "    num_lst = []\n",
    "    rectified_lst = []\n",
    "    if len(phone_number) < 10:\n",
    "        return \"Invalid Phone Number\"\n",
    "    else:\n",
    "        if match:\n",
    "            num_lst = convert_to_lst(phone_number,match.group())\n",
    "        else:   \n",
    "            num_lst = [phone_number]\n",
    "        \n",
    "        rectified_lst = validate_and_remove_problem_chars(num_lst)\n",
    "        \n",
    "        if len(rectified_lst) == 1:\n",
    "            return rectified_lst[0]\n",
    "        else:\n",
    "            return rectified_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_lst(phone_number,split_val):\n",
    "    '''This function is written to handle \n",
    "    multiple phone numbers scenario'''\n",
    "    num_lst = phone_number.split(split_val)\n",
    "    nw_lst = []\n",
    "    nw_lst.append(num_lst[0])\n",
    "    for i in range(1,len(num_lst)):\n",
    "        if len(num_lst[i]) < 10:\n",
    "            new_num = num_lst[0][:len(num_lst[0])-len(num_lst[i])] + num_lst[i]\n",
    "            nw_lst.append(new_num)\n",
    "        else:\n",
    "            nw_lst.append(num_lst[i])\n",
    "    \n",
    "    return nw_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_and_remove_problem_chars(num_lst):\n",
    "    '''This function is written to validate a \n",
    "    given phone number aganist standard format'''\n",
    "    correct_format = re.compile(r'^(\\+91) \\d{2} \\d{4} \\d{4}')\n",
    "    new_lst = []\n",
    "    for number in num_lst:\n",
    "        match = re.search(correct_format,number)\n",
    "        if match :\n",
    "            new_lst.append(number)\n",
    "        else : \n",
    "            new_number = change_to_standard_format(number)\n",
    "            new_lst.append(new_number)\n",
    "    \n",
    "    return new_lst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_to_standard_format(phone_number):\n",
    "    '''This function is written to convert a given phone number to standard format'''\n",
    "    new_number = phone_number.replace('(','').replace(')','').replace('-','').replace(' ','')\n",
    "    if new_number.startswith('+91'):\n",
    "        new_number = '+91 ' + new_number[3:5] + ' ' + new_number[5:9] + ' ' +new_number[9:14]\n",
    "    elif new_number.startswith('91'):\n",
    "        new_number = '+91 ' + new_number[2:4] + ' ' + new_number[4:8] + ' ' +new_number[8:13]\n",
    "    elif new_number.startswith('0'):\n",
    "        new_number = '+91 ' + new_number[1:3] + ' ' + new_number[3:7] + ' ' +new_number[7:12]\n",
    "    else:\n",
    "        new_number = '+91 ' + new_number[:2] + ' ' + new_number[2:6] + ' ' +new_number[6:11]\n",
    "    return new_number          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As output is large, I am writing it to an external file **\"correct_ph_numbers.txt\".** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_data(phone_numbers,rectify_phone_number,\"correct_ph_numbers.txt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems Encountered In Street Names <a name=\"streetNames\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now lets audit what are the different street names present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "street_names = process_tags(INPUT_FILENAME,\"addr:street\")\n",
    "output_data(street_names,filename=\"street_names.txt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The observations that can be drawn from street data are :\n",
    "  1. Most of the street names are either ending with word **road or marg** which are in correct format.\n",
    "  2. Few street names are in incorrect format i.e, they are having city, country name in them for eg.**\"Uttamnagar, Ahmedabad\".**\n",
    "  3. One of the Street names is mentioned in local language **\"Hindi\"** like **\"एरपोर्ट रोड\"**.\n",
    "  4. Some of the street names are in lower case letters, some of them are in upper case.\n",
    "  \n",
    "- Now lets write a function that would process a given street name and convert it into a standard format.\n",
    "- Lets create a dictionary that would contain incorrect names as keys and correct names as values, if a given street name is  found in that dictionary, we will return the correct value , else will return the given street name in standard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rectify_street_name(street_name):\n",
    "    incorrect_names = {'एरपोर्ट रोड': 'Airport Road'}\n",
    "    if street_name in incorrect_names:\n",
    "        return incorrect_names[street_name]\n",
    "    else:\n",
    "        new_street_name = street_name.lower().strip(' ')        \n",
    "        if new_street_name.endswith('ahmedabad'):\n",
    "            new_street_name = new_street_name.replace('ahmedabad','').replace(',','')\n",
    "        elif new_street_name.endswith('gujarat, india'):\n",
    "            new_street_name = new_street_name.replace('gujarat, india','').replace(',','')  \n",
    "        return new_street_name.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As output will be large, I am writing it to an external file **\"correct_street_names.txt\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_data(street_names,rectify_street_name,\"correct_street_names.txt\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems Encountered In Cuisine Data <a name=\"cuisineData\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now lets audit data present in **\"cuisine\"** tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Coffee and Snacks',\n",
       " 'Gujarati',\n",
       " 'Punjabi,_SouthIndia,_Gujarati Thali.',\n",
       " 'burger',\n",
       " 'burger;sandwich;regional;ice_cream;grill;cake;coffee_shop;pasta;noodles;pancake;pizza;chicken;fish_and_chips;curry;indian;vegan;fish;breakfast;savory_pancakes;tea;seafood;sausage;local;barbecue;vegetarian',\n",
       " 'ice_cream',\n",
       " 'indian',\n",
       " 'international',\n",
       " 'italian',\n",
       " 'multicuisine',\n",
       " 'pizza',\n",
       " 'regional',\n",
       " 'sandwich',\n",
       " 'sandwich;regional;cake;coffee_shop;asian;pasta;noodles;pancake;pizza;indian;mexican;sausage;tea;italian;barbecue;vegetarian',\n",
       " 'vegetarian'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuisines = process_tags(INPUT_FILENAME,\"cuisine\")\n",
    "(cuisines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The observations that can be drawn from the cuisine data are:\n",
    "    1. Few of the tags contain a single value.\n",
    "    2. Few of the tags contain more than one value .\n",
    "    \n",
    "    \n",
    "- Lets write a function that does the following:\n",
    "    1. If cuisine tag has a single value, it will return that value.\n",
    "    2. If cuisine tag has more than one value,it will return list of those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pizza ----> ['Pizza']\n",
      "vegetarian ----> ['Vegetarian']\n",
      "indian ----> ['Indian']\n",
      "italian ----> ['Italian']\n",
      "burger ----> ['Burger']\n",
      "Punjabi,_SouthIndia,_Gujarati Thali. ----> ['Punjabi', '_Southindia', '_Gujarati Thali.']\n",
      "regional ----> ['Regional']\n",
      "burger;sandwich;regional;ice_cream;grill;cake;coffee_shop;pasta;noodles;pancake;pizza;chicken;fish_and_chips;curry;indian;vegan;fish;breakfast;savory_pancakes;tea;seafood;sausage;local;barbecue;vegetarian ----> ['Burger', 'Sandwich', 'Regional', 'Ice_Cream', 'Grill', 'Cake', 'Coffee_Shop', 'Pasta', 'Noodles', 'Pancake', 'Pizza', 'Chicken', 'Fish_And_Chips', 'Curry', 'Indian', 'Vegan', 'Fish', 'Breakfast', 'Savory_Pancakes', 'Tea', 'Seafood', 'Sausage', 'Local', 'Barbecue', 'Vegetarian']\n",
      "international ----> ['International']\n",
      "sandwich;regional;cake;coffee_shop;asian;pasta;noodles;pancake;pizza;indian;mexican;sausage;tea;italian;barbecue;vegetarian ----> ['Sandwich', 'Regional', 'Cake', 'Coffee_Shop', 'Asian', 'Pasta', 'Noodles', 'Pancake', 'Pizza', 'Indian', 'Mexican', 'Sausage', 'Tea', 'Italian', 'Barbecue', 'Vegetarian']\n",
      "ice_cream ----> ['Ice_Cream']\n",
      "multicuisine ----> ['Multicuisine']\n",
      "Coffee and Snacks ----> ['Coffee And Snacks']\n",
      "Gujarati ----> ['Gujarati']\n",
      "sandwich ----> ['Sandwich']\n"
     ]
    }
   ],
   "source": [
    "def rectify_cuisine_data(cuisine):\n",
    "    cuisine_pattern = re.compile(r'[,;]')\n",
    "    match = re.search(cuisine_pattern,cuisine)\n",
    "    \n",
    "    if match:\n",
    "        cuisine_lst = cuisine.split(match.group())\n",
    "        return [cuisine.title() for cuisine in cuisine_lst]\n",
    "    \n",
    "    else:\n",
    "        return [cuisine.title()]\n",
    "    \n",
    "output_data(cuisines,func=rectify_cuisine_data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now lets write a function that will convert the xml dataset to json documents, which can be later be inserted to mongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "EXPECTED = [\"amenity\",\"cuisine\",\"name\",\"phone\",\"religion\",\"atm\",'building','landuse',\n",
    "            'highway','surface','lanes','bridge','maxspeed','leisure','sport','operator']\n",
    "\n",
    "speed = re.compile(r'(\\d)*')\n",
    "\n",
    "def create_element(element):\n",
    "    '''This function is written to convert each xml tag to a json document'''\n",
    "    \n",
    "    node = {}\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        # YOUR CODE HERE\n",
    "        created_dict = {}\n",
    "        attributes = element.attrib\n",
    "        pos = []\n",
    "        for k,v in attributes.items():\n",
    "            if k in CREATED :\n",
    "                created_dict[k] = v\n",
    "            else:\n",
    "                if k not in [\"lat\",\"lon\"]:\n",
    "                    node[k] = v\n",
    "        node[\"type\"] = element.tag\n",
    "        \n",
    "        if \"lat\" in attributes and \"lon\" in attributes:\n",
    "            node[\"pos\"] = [float(attributes[\"lat\"]),float(attributes[\"lon\"])]     \n",
    "        node[\"created\"] = created_dict \n",
    "        \n",
    "        node_refs = []\n",
    "        address = {}\n",
    "        \n",
    "        for elem in element.iter('nd'):\n",
    "            node_refs.append(elem.attrib[\"ref\"])\n",
    "            \n",
    "        for elem in element.iter('tag'):\n",
    "            tag_k = elem.attrib['k']\n",
    "            tag_v = elem.attrib['v']\n",
    "\n",
    "            \n",
    "            if tag_k == \"postal_code\":\n",
    "                tag_k = \"addr:postcode\"\n",
    "                                            \n",
    "            if tag_k.startswith('addr:') and  tag_k.count(':') == 1:\n",
    "                \n",
    "                if tag_k == \"addr:postcode\" and tag_v not in correct_postal_code_set:\n",
    "                    tag_v = None\n",
    "                \n",
    "                elif tag_k == \"addr:city\" :\n",
    "                    tag_v = rectify_city_name(tag_v)\n",
    "                 \n",
    "                elif tag_k == \"addr:street\":\n",
    "                    tag_v = rectify_street_name(tag_v)\n",
    "                \n",
    "                \n",
    "                if tag_v != None:\n",
    "                    address[tag_k.split(':')[1]] = tag_v\n",
    "                \n",
    "            elif tag_k in EXPECTED:\n",
    "                \n",
    "                if tag_k == \"phone\":\n",
    "                    tag_v = rectify_phone_number(tag_v)\n",
    "                    if tag_v == \"Invalid Phone Number\":\n",
    "                        tag_v = None\n",
    "                        \n",
    "                elif tag_k == \"cuisine\":\n",
    "                    tag_v = rectify_cuisine_data(tag_v)\n",
    "                    \n",
    "                elif tag_k == \"maxspeed\" or tag_k == 'lanes':\n",
    "                    match = re.search(speed,tag_v)\n",
    "                    tag_v = int(match.group())\n",
    "                \n",
    "                if tag_v != None:            \n",
    "                    node[tag_k] = tag_v\n",
    "                                \n",
    "        if len(node_refs) !=0 :\n",
    "            node[\"node_refs\"] = node_refs\n",
    "            \n",
    "        if len(address) != 0:\n",
    "            node[\"address\"] = address\n",
    "    \n",
    "        \n",
    "        return node\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_map(file_in, pretty = False):\n",
    "    '''This function is written to create json files'''\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = create_element(element)\n",
    "            if el:\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_map(INPUT_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling With DB and File Sizes <a name=\"dbFileSizes\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = pymongo.MongoClient('localhost:27017')\n",
    "db_name = 'openstreetmap'\n",
    "collection_name = 'ahmedabadData'\n",
    "file_name = 'ahmedabad_india1.osm.json'\n",
    "\n",
    "db = client[db_name]\n",
    "ahmedabad_osm = db[collection_name]\n",
    "    \n",
    "if collection_name in db.collection_names():\n",
    "    ahmedabad_osm.drop()\n",
    "\n",
    "cmd = \"mongoimport --db \" + db_name +' --collection ' + collection_name + ' --file ' +   file_name\n",
    "subprocess.call(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. File sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the input file: 108.54 MB\n",
      "Size of the ouput Json file: 128.03 MB\n"
     ]
    }
   ],
   "source": [
    "def getSize(filename):\n",
    "    input_file_size_b = os.path.getsize(filename)\n",
    "    input_file_siz_mb = (input_file_size_b)/(1024*1024)\n",
    "    return round(input_file_siz_mb,2)\n",
    "\n",
    "print(\"Size of the input file: {0} MB\".format(getSize(INPUT_FILENAME)))\n",
    "print(\"Size of the ouput Json file: {0} MB\".format(getSize(\"ahmedabad_india1.osm.json\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number Of Records:  627356\n"
     ]
    }
   ],
   "source": [
    "total_records = ahmedabad_osm.find().count()\n",
    "print('Total Number Of Records: ',total_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes:  546085\n"
     ]
    }
   ],
   "source": [
    "total_nodes = ahmedabad_osm.find({\"type\":\"node\"}).count()\n",
    "print('Number of nodes: ',total_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Number of ways :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ways:  81271\n"
     ]
    }
   ],
   "source": [
    "total_nodes = ahmedabad_osm.find({\"type\":\"way\"}).count()\n",
    "print('Number of ways: ',total_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5. Number of unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users contributed:  349\n"
     ]
    }
   ],
   "source": [
    "distinct_users = len(ahmedabad_osm.distinct('created.user'))\n",
    "print('Number of users contributed: ',distinct_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Top 3 users who contributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries uday01 made : 177332\n",
      "Total number of entries sramesh made : 136709\n",
      "Total number of entries chaitanya110 made : 123138\n"
     ]
    }
   ],
   "source": [
    "top_three_users = ahmedabad_osm.aggregate([{\"$group\":{\"_id\":\"$created.user\",\n",
    "                                   \"count\":{\"$sum\":1}}},\n",
    "                        {\"$sort\":{\"count\":-1}},\n",
    "                        {\"$limit\":3}])\n",
    "\n",
    "for user in top_three_users:\n",
    "    print(\"Total number of entries {0} made : {1}\".format(user[\"_id\"],user[\"count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Data Exploration With MongoDB <a name=\"furtherDbMongo\"></a>\n",
    "\n",
    "### 1. Top 5  Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Place_Of_Worship's Present : 92\n",
      "Total number of Restaurant's Present : 59\n",
      "Total number of Hospital's Present : 48\n",
      "Total number of School's Present : 43\n",
      "Total number of Bank's Present : 33\n"
     ]
    }
   ],
   "source": [
    "top_five_amenities = ahmedabad_osm.aggregate([{\"$match\":{\"amenity\":{\"$exists\":1}}},\n",
    "                                             {\"$group\":{\"_id\":\"$amenity\",\n",
    "                                                       \"count\":{\"$sum\":1}}},\n",
    "                                             {\"$sort\":{\"count\":-1}},\n",
    "                                             {\"$limit\":5}])\n",
    "\n",
    "for amenity in top_five_amenities:\n",
    "    print(\"Total number of {0}'s Present : {1}\".format(amenity[\"_id\"].title(),amenity[\"count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Number Of Atm's Present\n",
    "- Some of the banks have atms too, so to count the total number, we need to include both atm's and banks with atm's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Atms's :38\n"
     ]
    }
   ],
   "source": [
    "total_atms = ahmedabad_osm.find({\"$or\":[{\"amenity\":\"bank\",\"atm\":\"yes\"},{\"amenity\":\"atm\"}]}).count()\n",
    "print(\"Total Number of Atms's :{0}\".format(total_atms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Top 5 Cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Regional', 'Indian', 'Pizza', 'Vegetarian', 'Ice_Cream']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_five_cuisines = ahmedabad_osm.aggregate([{\"$unwind\":\"$cuisine\"},\n",
    "                                             {\"$group\":{\"_id\":\"$cuisine\",\n",
    "                                                       \"count\":{\"$sum\":1}}},\n",
    "                                            {\"$sort\":{\"count\":-1}},\n",
    "                                            {\"$limit\":5}])\n",
    "\n",
    "[cuisine[\"_id\"].title() for cuisine in top_five_cuisines]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Religions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hindu', 'Jain', 'Christian', 'Muslim', 'Nonsectarian', 'Zoroastrian']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[val.title() for val in ahmedabad_osm.distinct('religion')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Top 5 Building Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commercial: 138\n",
      "Apartments: 105\n",
      "House: 104\n",
      "Residential: 88\n",
      "University: 29\n"
     ]
    }
   ],
   "source": [
    "top_five_building_types = ahmedabad_osm.aggregate([{\"$match\":{\"building\":{\"$exists\":1,\"$ne\":\"yes\"}}},\n",
    "                                                   {\"$group\":{\"_id\":\"$building\",\n",
    "                                                       \"count\":{\"$sum\":1}}},\n",
    "                                                    {\"$sort\":{\"count\":-1}},\n",
    "                                                    {\"$limit\":5}])\n",
    "for building_type in top_five_building_types:\n",
    "    print(\"{0}: {1}\".format(building_type[\"_id\"].title(),building_type[\"count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Top 3 Landuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residential: 319\n",
      "Commercial: 89\n",
      "Industrial: 53\n"
     ]
    }
   ],
   "source": [
    "top_three_landuses = ahmedabad_osm.aggregate([{\"$match\":{\"landuse\":{\"$exists\":1}}},\n",
    "                                               {\"$group\":{\"_id\":\"$landuse\",\"count\":{\"$sum\":1}}},\n",
    "                                               {\"$sort\":{\"count\":-1}},\n",
    "                                               {\"$limit\":3}])\n",
    "\n",
    "for landuse in top_three_landuses:\n",
    "    print(\"{0}: {1}\".format(landuse[\"_id\"].title(),landuse[\"count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Leisure Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sports_centre', 'park', 'garden', 'swimming_pool', 'pitch', 'stadium', 'playground', 'recreation_ground', 'track', 'golf_course']\n"
     ]
    }
   ],
   "source": [
    "print(ahmedabad_osm.distinct('leisure'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Sport Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['multi', 'swimming', 'basketball', 'volleyball', 'cricket', 'tennis', 'skating', 'cricket,_football', 'running', 'soccer']\n"
     ]
    }
   ],
   "source": [
    "print(ahmedabad_osm.distinct('sport'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Way Tag Data Exploration\n",
    "#### 1. Highway Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crossing', 'traffic_signals', 'bus_stop', 'turning_circle', 'residential', 'street_lamp', 'rest_area', 'unclassified', 'primary', 'motorway', 'tertiary', 'pedestrian', 'trunk', 'footway', 'secondary', 'service', 'primary_link', 'living_street', 'motorway_link', 'trunk_link', 'tertiary_link', 'path', 'cycleway', 'secondary_link', 'track', 'road', 'steps', 'construction', 'bridleway']\n"
     ]
    }
   ],
   "source": [
    "highway_types = ahmedabad_osm.distinct('highway')\n",
    "print(highway_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Surface Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asphalt: 117\n",
      "Paved: 81\n",
      "Unpaved: 9\n",
      "Concrete: 6\n",
      "Gravel: 3\n",
      "Paving_Stones: 3\n",
      "Concrete:Plates: 3\n",
      "Ground: 2\n",
      "Grass: 2\n",
      "Sand: 1\n",
      "Metalroad: 1\n",
      "Dirt: 1\n"
     ]
    }
   ],
   "source": [
    "surface_types = ahmedabad_osm.aggregate([{\"$match\":{\"surface\":{\"$exists\":1}}},\n",
    "                        {\"$group\":{\"_id\":\"$surface\",\"count\":{\"$sum\":1}}},\n",
    "                         {\"$sort\":{\"count\":-1}}])\n",
    "\n",
    "for type in surface_types:\n",
    "    print(\"{0}: {1}\".format(type[\"_id\"].title(),type[\"count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Bridges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['132 Ft. Ring Road', 'Ahmadabad Vadodara Expressway', 'Anupam Bridge | Kankaria Railway Overbridge', 'Broken Bridge', 'Brts Route', 'Chamunda Flyover', 'Chandlodia Bridge', 'Chimanbhai Patel  Bridge', 'Ellisbridge', 'Fernandiz Bridge', 'Gandhi Bridge', 'Gandhinagar-Ahmedabad Highway', 'Gota Road', 'Gulbai Tekra Road', 'Iim Overbridge', 'Indira Bridge', 'Isckon Overbridge', 'Jamalpur Flyover Bridge', 'Kavi Nanalal Marg', 'Naroda Road', 'Nathalal Jhagadia Bridge', 'Nehru Bridge', 'Nh8A', 'Pirana Chandranagar Bridge', 'Proposed  Railway Overbridge', 'Proposed Railway Overbridge', 'Railway Foot Over Bridge', 'Rakhial Odhav Road', 'Rakhial Road', 'Rana Pratap Marg', 'Rishi Dadhichi Bridge', 'Sabarmati Riverfront Road', 'Sardar Bridge', 'Sardar Patel Ring Road', 'Small Bridge', 'Sola Road', 'Subhash Bridge', 'Western Railway Line']\n"
     ]
    }
   ],
   "source": [
    "bridges = ahmedabad_osm.aggregate([{\"$match\":{\"bridge\":\"yes\"}},\n",
    "                        {\"$group\":{\"_id\":\"$name\"}}])\n",
    "\n",
    "print(sorted([bridge[\"_id\"].title() for bridge in bridges if bridge[\"_id\"] != None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.  Top 2 MaxSpeeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gulbai Tekra Road: 120 Mph\n",
      "Ahmadabad Vadodara Expressway: 100 Mph\n"
     ]
    }
   ],
   "source": [
    "max_speeds = ahmedabad_osm.aggregate([{\"$match\":{\"maxspeed\":{\"$exists\":1}}},\n",
    "                                      {\"$group\":{\"_id\":\"$name\",\"speed\":{\"$max\":\"$maxspeed\"}}},\n",
    "                                      {\"$sort\":{\"speed\":-1}},\n",
    "                                      {\"$limit\":2}])\n",
    "for speed in max_speeds:\n",
    "    print(\"{0}: {1} Mph\".format(speed[\"_id\"].title(),speed[\"speed\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Roads With Max Lanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nh8: 4 Lanes\n",
      "Vatwa Road: 4 Lanes\n"
     ]
    }
   ],
   "source": [
    "max_lanes = ahmedabad_osm.aggregate([{\"$match\":{\"lanes\":{\"$exists\":1}}},\n",
    "                                      {\"$group\":{\"_id\":\"$name\",\"speed\":{\"$max\":\"$lanes\"}}},\n",
    "                                      {\"$sort\":{\"speed\":-1}},\n",
    "                                      {\"$limit\":2}])\n",
    "for lane in max_lanes:\n",
    "    print(\"{0}: {1} Lanes\".format(lane[\"_id\"].title(),lane[\"speed\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geospatial Indexing\n",
    "\n",
    "- The below function will return the nearby matches for a given amenity and position co-ordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def return_nearby_amenities(nearby_amenity,pos_list,nearby_limit):\n",
    "    ahmedabad_osm.create_index([('pos',pymongo.GEO2D),('amenity',1)])\n",
    "    nearby_data = ahmedabad_osm.find({\"pos\":{\"$near\":pos_list},\"amenity\":nearby_amenity}).limit(nearby_limit)\n",
    "    \n",
    "    for val in nearby_data:\n",
    "        print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Near By Banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('59314f949e4327d8804d3660'), 'id': '3500459657', 'type': 'node', 'pos': [23.0835179, 72.5909029], 'created': {'version': '1', 'timestamp': '2015-05-06T06:50:43Z', 'changeset': '30832181', 'uid': '2893945', 'user': 'Nikita Agarwal'}, 'atm': 'yes', 'name': 'HDFC Bank', 'amenity': 'bank'}\n",
      "{'_id': ObjectId('59314f949e4327d8804d3668'), 'id': '3500459666', 'type': 'node', 'pos': [23.0825491, 72.5900739], 'created': {'version': '1', 'timestamp': '2015-05-06T06:50:44Z', 'changeset': '30832181', 'uid': '2893945', 'user': 'Nikita Agarwal'}, 'atm': 'yes', 'name': 'Bank of Baroda', 'amenity': 'bank'}\n",
      "{'_id': ObjectId('59314f949e4327d8804d3663'), 'id': '3500459659', 'type': 'node', 'pos': [23.0834501, 72.5888148], 'created': {'version': '1', 'timestamp': '2015-05-06T06:50:44Z', 'changeset': '30832181', 'uid': '2893945', 'user': 'Nikita Agarwal'}, 'name': 'Gol Bank', 'amenity': 'bank'}\n"
     ]
    }
   ],
   "source": [
    "return_nearby_amenities('bank',[23.0945918,72.6119846],3)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Near By Hospitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('59314f939e4327d8804c78a1'), 'id': '611572691', 'type': 'node', 'pos': [23.0924908, 72.5917525], 'created': {'version': '1', 'timestamp': '2010-01-11T08:02:54Z', 'changeset': '3594021', 'uid': '217810', 'user': 'Shardendu'}, 'amenity': 'hospital'}\n",
      "{'_id': ObjectId('59314f949e4327d8804d365b'), 'id': '3500459652', 'type': 'node', 'pos': [23.0829627, 72.5906964], 'created': {'version': '1', 'timestamp': '2015-05-06T06:50:43Z', 'changeset': '30832181', 'uid': '2893945', 'user': 'Nikita Agarwal'}, 'name': 'Pukhraj Hospital', 'amenity': 'hospital', 'address': {'city': 'Ahmedabad', 'postcode': '380005'}}\n",
      "{'_id': ObjectId('59314f949e4327d8804d3657'), 'id': '3500454471', 'type': 'node', 'pos': [23.0836049, 72.5901213], 'created': {'version': '1', 'timestamp': '2015-05-06T06:40:57Z', 'changeset': '30831958', 'uid': '2893945', 'user': 'Nikita Agarwal'}, 'name': 'Geeta Maternity Hospital', 'amenity': 'hospital', 'address': {'city': 'Ahmedabad', 'postcode': '380005'}}\n"
     ]
    }
   ],
   "source": [
    "return_nearby_amenities('hospital',[23.0945918,72.6119846],3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Near By Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('59314fa19e4327d880547060'), 'id': '4481532989', 'type': 'node', 'pos': [23.111054, 72.6050699], 'created': {'version': '1', 'timestamp': '2016-11-04T11:14:31Z', 'changeset': '43398935', 'uid': '4112063', 'user': 'kailashdhirwani'}, 'name': 'AFC Garden Restaurant', 'amenity': 'restaurant'}\n",
      "{'_id': ObjectId('59314f939e4327d8804c789c'), 'id': '611572686', 'type': 'node', 'pos': [23.1057395, 72.5975686], 'created': {'version': '1', 'timestamp': '2010-01-11T08:02:53Z', 'changeset': '3594021', 'uid': '217810', 'user': 'Shardendu'}, 'amenity': 'restaurant'}\n",
      "{'_id': ObjectId('59314fa29e4327d880549420'), 'id': '4672030298', 'type': 'node', 'pos': [23.0687153, 72.5807241], 'created': {'version': '1', 'timestamp': '2017-02-07T14:13:46Z', 'changeset': '45886810', 'uid': '5276500', 'user': 'Silene Hoiry'}, 'amenity': 'restaurant'}\n"
     ]
    }
   ],
   "source": [
    "return_nearby_amenities('restaurant',[23.0945918,72.6119846],3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Near By Cinema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('59314f939e4327d8804c78a0'), 'id': '611572690', 'type': 'node', 'pos': [23.0923921, 72.5938017], 'created': {'version': '1', 'timestamp': '2010-01-11T08:02:54Z', 'changeset': '3594021', 'uid': '217810', 'user': 'Shardendu'}, 'amenity': 'cinema'}\n",
      "{'_id': ObjectId('59314fa29e4327d88054787e'), 'id': '4488103325', 'type': 'node', 'pos': [23.0810593, 72.6415185], 'created': {'version': '1', 'timestamp': '2016-11-08T06:52:17Z', 'changeset': '43480049', 'uid': '4112063', 'user': 'kailashdhirwani'}, 'name': 'Maya CINEMA -CLOSED', 'amenity': 'cinema'}\n",
      "{'_id': ObjectId('59314f949e4327d8804cebd3'), 'id': '2699807313', 'type': 'node', 'pos': [23.0664215, 72.5316963], 'created': {'version': '1', 'timestamp': '2014-03-03T12:25:12Z', 'changeset': '20887124', 'uid': '1964435', 'user': 'trishul'}, 'name': 'Rajhans Multiplex', 'amenity': 'cinema'}\n"
     ]
    }
   ],
   "source": [
    "return_nearby_amenities('cinema',[23.0945918,72.6119846],3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Conclusion <a name=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Though OpenStreetMap data seems to be fairly sorted in terms of the data distribution(nodes, ways and relations),Its becomes problematic when it comes to data representation in terms of tags due to following reasons:\n",
    "\n",
    "    1. Different tags have been used to represent the same data like \"postal_code\" and \"addr:postcode\".\n",
    "    2. Phone numbers are in different formats.\n",
    "    3. Street names are having city and states names appended.\n",
    "    4. City names are having local area names appended.\n",
    "    \n",
    "    \n",
    "- To avoid these problems OpenStreetMap should provide various templates based on the country, so that data would be more clean and sorted.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
